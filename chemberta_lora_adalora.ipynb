{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install deepchem transformers peft"
      ],
      "metadata": {
        "id": "9nzNqawxU3Mh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "logging.getLogger(\"deepchem\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "uFwAMby8ea04"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import deepchem as dc\n",
        "from peft import get_peft_model, LoraConfig, TaskType, AdaLoraConfig\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "z4GBncRqVVbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfd1988-83be-4a0b-e106-197c1a8500c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "cbsp5hH7kJyX",
        "outputId": "62ab64c4-88cc-4c4c-c43c-3e1b2b8bec26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths and parameters\n",
        "MODEL_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# LoRA Configuration\n",
        "LORA_R = 8  # Rank of LoRA\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1"
      ],
      "metadata": {
        "id": "Mj3k4DdTXm3t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClinToxDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, split='train', max_length=128):\n",
        "        \"\"\"\n",
        "        Custom PyTorch Dataset for the ClinTox dataset.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "\n",
        "        # Load ClinTox dataset from DeepChem\n",
        "        tasks, datasets, transformers = dc.molnet.load_clintox()\n",
        "        train_dataset, valid_dataset, test_dataset = datasets\n",
        "\n",
        "        # Convert to SMILES and labels\n",
        "        self.smiles_train, self.labels_train = self.remove_invalid_smiles(train_dataset.ids, train_dataset.y)\n",
        "        self.smiles_valid, self.labels_valid = self.remove_invalid_smiles(valid_dataset.ids, valid_dataset.y)\n",
        "        self.smiles_test, self.labels_test = self.remove_invalid_smiles(test_dataset.ids, test_dataset.y)\n",
        "\n",
        "        # Set active data split based on input parameter\n",
        "        if split == 'train':\n",
        "            self.smiles = self.smiles_train\n",
        "            self.labels = self.labels_train\n",
        "        elif split == 'valid':\n",
        "            self.smiles = self.smiles_valid\n",
        "            self.labels = self.labels_valid\n",
        "        elif split == 'test':\n",
        "            self.smiles = self.smiles_test\n",
        "            self.labels = self.labels_test\n",
        "        else:\n",
        "            raise ValueError(\"Invalid split. Use 'train', 'valid', or 'test'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "    def remove_invalid_smiles(self, smiles, labels):\n",
        "        \"\"\"\n",
        "        Filters out invalid SMILES strings using RDKit validation.\n",
        "        \"\"\"\n",
        "        valid_indices = []\n",
        "        for i, smile in enumerate(smiles):\n",
        "            try:\n",
        "                mol = Chem.MolFromSmiles(smile)\n",
        "                if mol is not None:\n",
        "                    valid_indices.append(i)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return smiles[valid_indices], labels[valid_indices]"
      ],
      "metadata": {
        "id": "cVUIwWpkXm0m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation or test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits.squeeze(-1)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_preds.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Binary predictions for accuracy\n",
        "    bin_preds = (all_preds > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(all_labels, bin_preds)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "cxRAKno7Xmsl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    all_params = sum(p.numel() for p in model.parameters())\n",
        "    percentage_trained = round(100*trainable_params/all_params, 2)\n",
        "    print(f\"Trainable: {trainable_params} | All: {all_params} | % Trained: {percentage_trained}\")\n",
        "    return trainable_params, all_params, percentage_trained"
      ],
      "metadata": {
        "id": "rMyvPobUn8oo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_profile(model, tokenizer, optimization_name, lr=LEARNING_RATE):\n",
        "    \"\"\"\n",
        "    Training Loop with Profiling\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n--- {optimization_name} - Trainable Parameters ---\")\n",
        "    trainable_params, all_params, percentage_trained = print_trainable_parameters(model)\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"testing\",\n",
        "        entity=\"hpml-proj-deepchem\",\n",
        "        name=f\"{optimization_name}_lr{lr}\",\n",
        "        config={\n",
        "            \"model_name\": MODEL_NAME,\n",
        "            \"optimization\": optimization_name,\n",
        "            \"learning_rate\": lr,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"max_length\": MAX_LENGTH,\n",
        "            \"lora_r\": LORA_R,\n",
        "            \"lora_alpha\": LORA_ALPHA,\n",
        "            \"lora_dropout\": LORA_DROPOUT,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    wandb.log({\n",
        "        \"trainable_parameters\": trainable_params,\n",
        "        \"total_parameters\": all_params,\n",
        "        \"parameter_efficiency\": percentage_trained\n",
        "    })\n",
        "\n",
        "    # Create datasets for each split\n",
        "    train_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"train\", max_length=MAX_LENGTH)\n",
        "    val_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"valid\", max_length=MAX_LENGTH)\n",
        "    test_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"test\", max_length=MAX_LENGTH)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "    print(f\"\\n--- {optimization_name} ---\")\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits.squeeze(-1)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        val_loss, val_acc = evaluate_model(model, val_dataloader, device)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"epoch_time\": epoch_time,\n",
        "            \"model_type\": optimization_name\n",
        "        })\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Training Time: {epoch_time:.2f} s | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    test_loss, test_acc = evaluate_model(model, test_dataloader, device)\n",
        "    training_time = round(end_time - start_time, 2)\n",
        "    print(f\"\\n--- {optimization_name} ---\")\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"training_time\": training_time,\n",
        "        \"trainable_parameters\": trainable_params,\n",
        "        \"total_parameters\": all_params,\n",
        "        \"parameter_efficiency\": percentage_trained,\n",
        "        \"model_type\": optimization_name\n",
        "    })\n",
        "\n",
        "    wandb.log({\n",
        "        f\"{optimization_name}_train_loss_curve\": wandb.plot.line_series(\n",
        "            xs=list(range(1, EPOCHS+1)),\n",
        "            ys=[train_losses],\n",
        "            keys=[f\"{optimization_name}_train_loss\"],\n",
        "            title=f\"{optimization_name} Training Loss\",\n",
        "            xname=\"Epoch\"\n",
        "        ),\n",
        "        f\"{optimization_name}_val_loss_curve\": wandb.plot.line_series(\n",
        "            xs=list(range(1, EPOCHS+1)),\n",
        "            ys=[val_losses],\n",
        "            keys=[f\"{optimization_name}_val_loss\"],\n",
        "            title=f\"{optimization_name} Validation Loss\",\n",
        "            xname=\"Epoch\"\n",
        "        )\n",
        "    })\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"val_accuracies\": val_accuracies,\n",
        "        \"training_time\": training_time,\n",
        "        \"test_accuracy\": test_acc}"
      ],
      "metadata": {
        "id": "o55jzgLvYCn4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_tuning(model_type=\"LoRA\", learning_rates=[1e-5, 2e-5, 3e-5, 5e-5, 1e-4, 3e-4]):\n",
        "    \"\"\"\n",
        "    Run hyperparameter tuning  with wandb tracking\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the main sweep run\n",
        "    parent_run = wandb.init(project=\"testing\", entity=\"hpml-proj-deepchem\", name=f\"{model_type}_Hyperparameter_Sweep\", job_type=\"sweep\")\n",
        "\n",
        "    results = {\n",
        "        \"learning_rate\": [],\n",
        "        \"test_accuracy\": [],\n",
        "        \"training_time\": [],\n",
        "    }\n",
        "\n",
        "    # Run experiment for each learning rate\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\n--- {model_type} with learning_rate={lr} ---\")\n",
        "\n",
        "        # new model for each run\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = None\n",
        "\n",
        "        if model_type.lower() == \"lora\":\n",
        "            model = setup_lora_model(MODEL_NAME)\n",
        "        elif model_type.lower() == \"adalora\":\n",
        "            model = setup_adalora_model(MODEL_NAME)\n",
        "\n",
        "        with wandb.init(\n",
        "            project=\"testing\",\n",
        "            entity=\"hpml-proj-deepchem\",\n",
        "            name=f\"{model_type}_lr_{lr}\",\n",
        "            group=f\"{model_type}_Sweep\",\n",
        "            job_type=\"run\",\n",
        "            config={\"learning_rate\": lr},\n",
        "            reinit=True  # Allows multiple init calls\n",
        "        ):\n",
        "\n",
        "            metrics = train_and_profile(model, tokenizer, f\"{model_type}_lr_{lr}\", lr=lr)\n",
        "\n",
        "            results[\"learning_rate\"].append(lr)\n",
        "            results[\"test_accuracy\"].append(metrics[\"test_accuracy\"])\n",
        "            results[\"training_time\"].append(metrics[\"training_time\"])\n",
        "\n",
        "        # Clean up memory\n",
        "        del model, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "    wandb.finish()\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Create and log summary visualization\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.plot(results_df[\"learning_rate\"], results_df[\"test_accuracy\"], 'bo-')\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Learning Rate')\n",
        "    ax.set_ylabel('Test Accuracy')\n",
        "    ax.set_title(f'{model_type} Performance vs Learning Rate')\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Add text annotations for each point\n",
        "    for i, lr in enumerate(results_df[\"learning_rate\"]):\n",
        "        ax.annotate(\n",
        "            f\"Time: {results_df['training_time'][i]:.1f}s\\nAcc: {results_df['test_accuracy'][i]:.4f}\",\n",
        "            (lr, results_df[\"test_accuracy\"][i]),\n",
        "            textcoords=\"offset points\",\n",
        "            xytext=(0, 10),\n",
        "            ha='center'\n",
        "        )\n",
        "\n",
        "    second_run = wandb.init(project=\"testing\", entity=\"hpml-proj-deepchem\", name=f\"{model_type}_Hyperparameter_GraphTable\")\n",
        "\n",
        "    wandb.log({f\"{model_type}_lr_tuning_curve\": wandb.Image(fig),\n",
        "               f\"{model_type}_lr_tuning_table\": wandb.Table(dataframe=results_df)})\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "4SR2sBI0ZI4B"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base ChemBeRTa Model Without Any Additional Fine-Tuning"
      ],
      "metadata": {
        "id": "TtgrWa1VepCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=2,  # Binary classification for ClinTox\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "test_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"test\", max_length=MAX_LENGTH)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "_, test_acc = evaluate_model(baseline, test_dataloader, device)\n",
        "\n",
        "print(f\"Baseline (Not Fine-Tuned) Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-jt4zRXms1q",
        "outputId": "1ba140d5-a0b8-4449-f494-5eb8c2ed8845"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 21.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline (Not Fine-Tuned) Test Accuracy: 0.0946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base ChemBeRTa Model with Full Parameter Fine-Tuning\n"
      ],
      "metadata": {
        "id": "gPPDM-IleuBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_baseline_model(model_name):\n",
        "    \"\"\"\n",
        "    Set up a baseline model\n",
        "    \"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,  # Binary classification for ClinTox\n",
        "        return_dict=True\n",
        "    )\n",
        "    return model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model_baseline = setup_baseline_model(MODEL_NAME)\n",
        "\n",
        "baseline_metrics = train_and_profile(model_baseline, tokenizer, \"Model_Baseline_Finetuned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DFSO4MO4JuxW",
        "outputId": "e28390e4-d4a0-4392-d96d-b3da2ff08c9a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model_Baseline_Finetuned - Trainable Parameters ---\n",
            "Trainable: 44105474 | All: 44105474 | % Trained: 100.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>epoch_time</td><td>▁</td></tr><tr><td>parameter_efficiency</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>trainable_parameters</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>epoch_time</td><td>5.5166</td></tr><tr><td>model_type</td><td>Model_Baseline_Finet...</td></tr><tr><td>parameter_efficiency</td><td>100</td></tr><tr><td>total_parameters</td><td>44105474</td></tr><tr><td>train_loss</td><td>0.25829</td></tr><tr><td>trainable_parameters</td><td>44105474</td></tr><tr><td>val_accuracy</td><td>0.96622</td></tr><tr><td>val_loss</td><td>0.09241</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Model_Baseline_Finetuned_lr2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/fi94aaya' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/fi94aaya</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_203755-fi94aaya/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_203813-7e88frv7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/7e88frv7' target=\"_blank\">Model_Baseline_Finetuned_lr2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/7e88frv7' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/7e88frv7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model_Baseline_Finetuned ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.76it/s, loss=0.0674]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.47 s | Train Loss: 0.2631 | Val Loss: 0.0949 | Val Accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.69it/s, loss=0.0308]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.53 s | Train Loss: 0.0805 | Val Loss: 0.0953 | Val Accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.67it/s, loss=0.0129]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.55 s | Train Loss: 0.0534 | Val Loss: 0.1127 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.73it/s, loss=0.0978]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.50 s | Train Loss: 0.0492 | Val Loss: 0.0665 | Val Accuracy: 0.9865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.74it/s, loss=0.0265]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.49 s | Train Loss: 0.0350 | Val Loss: 0.0665 | Val Accuracy: 0.9865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model_Baseline_Finetuned ---\n",
            "Training time: 28.79 seconds\n",
            "Test Loss: 0.0504 | Test Accuracy: 0.9865\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▇█▃▃</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▂▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▅▅▁██</td></tr><tr><td>val_loss</td><td>▅▅█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.49387</td></tr><tr><td>model_type</td><td>Model_Baseline_Finet...</td></tr><tr><td>parameter_efficiency</td><td>100</td></tr><tr><td>test_accuracy</td><td>0.98649</td></tr><tr><td>test_loss</td><td>0.05038</td></tr><tr><td>total_parameters</td><td>44105474</td></tr><tr><td>train_loss</td><td>0.03498</td></tr><tr><td>trainable_parameters</td><td>44105474</td></tr><tr><td>training_time</td><td>28.79</td></tr><tr><td>val_accuracy</td><td>0.98649</td></tr><tr><td>val_loss</td><td>0.06651</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Model_Baseline_Finetuned_lr2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/7e88frv7' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/7e88frv7</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_203813-7e88frv7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChemBeRTa With LoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "irYHHy1mfAWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_lora_model(model_name):\n",
        "    \"\"\"\n",
        "    Set up a model with LoRA configuration\n",
        "    \"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,  # Binary classification for ClinTox\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        target_modules=[\"query\", \"value\"]  # Target attention modules\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nNFP2D9Osreq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_tuning(\"LoRA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zm65C-ZdWi89",
        "outputId": "049e53fc-bd51-4031-b0d4-83bb3a678ba1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210250-1hkxt83d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/1hkxt83d' target=\"_blank\">LoRA_Hyperparameter_Sweep</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/1hkxt83d' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/1hkxt83d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA with learning_rate=1e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_Hyperparameter_Sweep</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/1hkxt83d' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/1hkxt83d</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210250-1hkxt83d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210251-dfkbkmsd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/dfkbkmsd' target=\"_blank\">LoRA_lr_1e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/dfkbkmsd' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/dfkbkmsd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_1e-05 - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_1e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/dfkbkmsd' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/dfkbkmsd</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210251-dfkbkmsd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210253-myj3zlrc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/myj3zlrc' target=\"_blank\">LoRA_lr_1e-05_lr1e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/myj3zlrc' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/myj3zlrc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_1e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.35it/s, loss=0.355]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.96 s | Train Loss: 0.5158 | Val Loss: 0.3309 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:04<00:00,  9.24it/s, loss=0.297]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 4.01 s | Train Loss: 0.3263 | Val Loss: 0.2262 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:04<00:00,  9.15it/s, loss=0.601]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 4.05 s | Train Loss: 0.2668 | Val Loss: 0.1929 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:04<00:00,  9.17it/s, loss=0.305]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 4.04 s | Train Loss: 0.2411 | Val Loss: 0.1777 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:04<00:00,  9.18it/s, loss=0.479]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 4.04 s | Train Loss: 0.2240 | Val Loss: 0.1679 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_1e-05 ---\n",
            "Training time: 21.42 seconds\n",
            "Test Loss: 0.1781 | Test Accuracy: 0.9324\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▅█▇▇</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>4.03599</td></tr><tr><td>model_type</td><td>LoRA_lr_1e-05</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.93243</td></tr><tr><td>test_loss</td><td>0.1781</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.224</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>21.42</td></tr><tr><td>val_accuracy</td><td>0.9527</td></tr><tr><td>val_loss</td><td>0.16785</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_1e-05_lr1e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/myj3zlrc' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/myj3zlrc</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210253-myj3zlrc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA with learning_rate=2e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210320-aqe19oze</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/aqe19oze' target=\"_blank\">LoRA_lr_2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/aqe19oze' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/aqe19oze</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_2e-05 - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/aqe19oze' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/aqe19oze</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210320-aqe19oze/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210320-h4c2no5v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/h4c2no5v' target=\"_blank\">LoRA_lr_2e-05_lr2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/h4c2no5v' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/h4c2no5v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_2e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.38it/s, loss=0.25]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.95 s | Train Loss: 0.4945 | Val Loss: 0.2421 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:03<00:00,  9.35it/s, loss=0.223]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 3.96 s | Train Loss: 0.2646 | Val Loss: 0.1810 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:03<00:00,  9.47it/s, loss=0.31]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 3.91 s | Train Loss: 0.2242 | Val Loss: 0.1643 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:03<00:00,  9.44it/s, loss=0.12]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 3.92 s | Train Loss: 0.1966 | Val Loss: 0.1500 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:03<00:00,  9.45it/s, loss=0.161]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 3.92 s | Train Loss: 0.1683 | Val Loss: 0.1336 | Val Accuracy: 0.9595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_2e-05 ---\n",
            "Training time: 20.97 seconds\n",
            "Test Loss: 0.1236 | Test Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▆█▁▃▂</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁█</td></tr><tr><td>val_loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>3.91844</td></tr><tr><td>model_type</td><td>LoRA_lr_2e-05</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.9527</td></tr><tr><td>test_loss</td><td>0.1236</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.16826</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>20.97</td></tr><tr><td>val_accuracy</td><td>0.95946</td></tr><tr><td>val_loss</td><td>0.13361</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_2e-05_lr2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/h4c2no5v' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/h4c2no5v</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210320-h4c2no5v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA with learning_rate=3e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210347-lnvzgsav</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/lnvzgsav' target=\"_blank\">LoRA_lr_3e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/lnvzgsav' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/lnvzgsav</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_3e-05 - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_3e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/lnvzgsav' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/lnvzgsav</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210347-lnvzgsav/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210348-ee8gmzqg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/ee8gmzqg' target=\"_blank\">LoRA_lr_3e-05_lr3e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/ee8gmzqg' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/ee8gmzqg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_3e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.50it/s, loss=0.325]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.90 s | Train Loss: 0.3248 | Val Loss: 0.1776 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:03<00:00,  9.52it/s, loss=0.241]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 3.89 s | Train Loss: 0.2128 | Val Loss: 0.1559 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:03<00:00,  9.47it/s, loss=0.169]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 3.91 s | Train Loss: 0.1750 | Val Loss: 0.1360 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:03<00:00,  9.45it/s, loss=0.128]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 3.92 s | Train Loss: 0.1416 | Val Loss: 0.1148 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:03<00:00,  9.41it/s, loss=0.0553]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 3.94 s | Train Loss: 0.1151 | Val Loss: 0.1022 | Val Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_3e-05 ---\n",
            "Training time: 20.86 seconds\n",
            "Test Loss: 0.0899 | Test Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▂▁▄▆█</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▅█</td></tr><tr><td>val_loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>3.93504</td></tr><tr><td>model_type</td><td>LoRA_lr_3e-05</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.96622</td></tr><tr><td>test_loss</td><td>0.0899</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.1151</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>20.86</td></tr><tr><td>val_accuracy</td><td>0.97973</td></tr><tr><td>val_loss</td><td>0.10224</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_3e-05_lr3e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/ee8gmzqg' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/ee8gmzqg</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210348-ee8gmzqg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA with learning_rate=5e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210414-jrl0zoil</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/jrl0zoil' target=\"_blank\">LoRA_lr_5e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/jrl0zoil' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/jrl0zoil</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_5e-05 - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_5e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/jrl0zoil' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/jrl0zoil</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210414-jrl0zoil/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210415-40lstolv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/40lstolv' target=\"_blank\">LoRA_lr_5e-05_lr5e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/40lstolv' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/40lstolv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_5e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.47it/s, loss=0.343]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.91 s | Train Loss: 0.2888 | Val Loss: 0.1651 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:03<00:00,  9.46it/s, loss=0.152]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 3.91 s | Train Loss: 0.1770 | Val Loss: 0.1313 | Val Accuracy: 0.9595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:03<00:00,  9.39it/s, loss=0.284]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 3.95 s | Train Loss: 0.1270 | Val Loss: 0.1017 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:03<00:00,  9.37it/s, loss=0.0269]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 3.95 s | Train Loss: 0.0961 | Val Loss: 0.0899 | Val Accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:03<00:00,  9.31it/s, loss=0.0267]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 3.98 s | Train Loss: 0.0705 | Val Loss: 0.0962 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_5e-05 ---\n",
            "Training time: 21.01 seconds\n",
            "Test Loss: 0.0820 | Test Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▁▄▅█</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▃▆█▆</td></tr><tr><td>val_loss</td><td>█▅▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>3.97946</td></tr><tr><td>model_type</td><td>LoRA_lr_5e-05</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.96622</td></tr><tr><td>test_loss</td><td>0.08198</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.07049</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>21.01</td></tr><tr><td>val_accuracy</td><td>0.96622</td></tr><tr><td>val_loss</td><td>0.09619</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_5e-05_lr5e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/40lstolv' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/40lstolv</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210415-40lstolv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA with learning_rate=0.0001 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210441-qo99ajcd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/qo99ajcd' target=\"_blank\">LoRA_lr_0.0001</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/qo99ajcd' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/qo99ajcd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_0.0001 - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_0.0001</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/qo99ajcd' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/qo99ajcd</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210441-qo99ajcd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210442-dwxl33au</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/dwxl33au' target=\"_blank\">LoRA_lr_0.0001_lr0.0001</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/dwxl33au' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/dwxl33au</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_0.0001 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.47it/s, loss=0.136]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.91 s | Train Loss: 0.2770 | Val Loss: 0.1400 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:03<00:00,  9.40it/s, loss=0.0768]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 3.94 s | Train Loss: 0.1395 | Val Loss: 0.1051 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:03<00:00,  9.47it/s, loss=0.0708]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 3.91 s | Train Loss: 0.0855 | Val Loss: 0.1195 | Val Accuracy: 0.9459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:03<00:00,  9.42it/s, loss=0.142]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 3.93 s | Train Loss: 0.0670 | Val Loss: 0.0988 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:03<00:00,  9.35it/s, loss=0.121]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 3.96 s | Train Loss: 0.0568 | Val Loss: 0.0907 | Val Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 20.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_0.0001 ---\n",
            "Training time: 20.96 seconds\n",
            "Test Loss: 0.0740 | Test Accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▅▁▄█</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▄▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▂▅▁▅█</td></tr><tr><td>val_loss</td><td>█▃▅▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>3.96077</td></tr><tr><td>model_type</td><td>LoRA_lr_0.0001</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.97297</td></tr><tr><td>test_loss</td><td>0.07398</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.05678</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>20.96</td></tr><tr><td>val_accuracy</td><td>0.97973</td></tr><tr><td>val_loss</td><td>0.0907</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_0.0001_lr0.0001</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/dwxl33au' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/dwxl33au</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210442-dwxl33au/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA with learning_rate=0.0003 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210509-9rsmn2ud</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/9rsmn2ud' target=\"_blank\">LoRA_lr_0.0003</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/9rsmn2ud' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/9rsmn2ud</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_0.0003 - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_0.0003</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/9rsmn2ud' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/9rsmn2ud</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210509-9rsmn2ud/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210510-l5nqzjus</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/l5nqzjus' target=\"_blank\">LoRA_lr_0.0003_lr0.0003</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/l5nqzjus' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/l5nqzjus</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_0.0003 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.49it/s, loss=0.168]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.90 s | Train Loss: 0.1946 | Val Loss: 0.0884 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:03<00:00,  9.49it/s, loss=0.258]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 18.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 3.90 s | Train Loss: 0.0776 | Val Loss: 0.1014 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:03<00:00,  9.44it/s, loss=0.164]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 3.92 s | Train Loss: 0.0538 | Val Loss: 0.0812 | Val Accuracy: 0.9865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:03<00:00,  9.38it/s, loss=0.0548]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 3.95 s | Train Loss: 0.0498 | Val Loss: 0.0746 | Val Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:03<00:00,  9.40it/s, loss=0.0231]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 3.94 s | Train Loss: 0.0426 | Val Loss: 0.1310 | Val Accuracy: 0.9324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA_lr_0.0003 ---\n",
            "Training time: 20.93 seconds\n",
            "Test Loss: 0.0746 | Test Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▁▄█▇</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▅▅█▇▁</td></tr><tr><td>val_loss</td><td>▃▄▂▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>3.93989</td></tr><tr><td>model_type</td><td>LoRA_lr_0.0003</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.97973</td></tr><tr><td>test_loss</td><td>0.07459</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.04258</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>20.93</td></tr><tr><td>val_accuracy</td><td>0.93243</td></tr><tr><td>val_loss</td><td>0.13101</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr_0.0003_lr0.0003</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/l5nqzjus' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/l5nqzjus</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210510-l5nqzjus/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210536-u8zenyyf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/u8zenyyf' target=\"_blank\">LoRA_Hyperparameter_GraphTable</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/u8zenyyf' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/u8zenyyf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_Hyperparameter_GraphTable</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/u8zenyyf' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/u8zenyyf</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210536-u8zenyyf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   learning_rate  test_accuracy  training_time\n",
              "0        0.00001       0.932432          21.42\n",
              "1        0.00002       0.952703          20.97\n",
              "2        0.00003       0.966216          20.86\n",
              "3        0.00005       0.966216          21.01\n",
              "4        0.00010       0.972973          20.96\n",
              "5        0.00030       0.979730          20.93"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fae5ac29-3e5c-4792-b6b5-5d9f700eec27\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>training_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.932432</td>\n",
              "      <td>21.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.952703</td>\n",
              "      <td>20.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00003</td>\n",
              "      <td>0.966216</td>\n",
              "      <td>20.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00005</td>\n",
              "      <td>0.966216</td>\n",
              "      <td>21.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.972973</td>\n",
              "      <td>20.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00030</td>\n",
              "      <td>0.979730</td>\n",
              "      <td>20.93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fae5ac29-3e5c-4792-b6b5-5d9f700eec27')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fae5ac29-3e5c-4792-b6b5-5d9f700eec27 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fae5ac29-3e5c-4792-b6b5-5d9f700eec27');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-09e0a973-da33-49b4-9b04-3cd387f944f8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-09e0a973-da33-49b4-9b04-3cd387f944f8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-09e0a973-da33-49b4-9b04-3cd387f944f8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"hyperparameter_tuning(\\\"LoRA\\\")\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00011004544515789829,\n        \"min\": 1e-05,\n        \"max\": 0.0003,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1e-05,\n          2e-05,\n          0.0003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01691439941938611,\n        \"min\": 0.9324324324324325,\n        \"max\": 0.9797297297297297,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9527027027027027,\n          0.9797297297297297,\n          0.9662162162162162\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19987496091306764,\n        \"min\": 20.86,\n        \"max\": 21.42,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          21.42,\n          20.97,\n          20.93\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model_lora = setup_lora_model(MODEL_NAME)\n",
        "\n",
        "lora_metrics = train_and_profile(model_lora, tokenizer, \"LoRA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7UsVqPxdYCld",
        "outputId": "00ad26fe-f2c3-4aff-fc7a-986150d2aa62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA - Trainable Parameters ---\n",
            "Trainable: 739586 | All: 44845060 | % Trained: 1.65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210613-g6815405</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/g6815405' target=\"_blank\">LoRA_lr2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/g6815405' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/g6815405</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:03<00:00,  9.41it/s, loss=0.312]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 3.93 s | Train Loss: 0.4095 | Val Loss: 0.2168 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:03<00:00,  9.42it/s, loss=0.402]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 3.93 s | Train Loss: 0.2480 | Val Loss: 0.1750 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:03<00:00,  9.37it/s, loss=0.147]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 3.95 s | Train Loss: 0.2111 | Val Loss: 0.1588 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:03<00:00,  9.32it/s, loss=0.237]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 3.97 s | Train Loss: 0.1818 | Val Loss: 0.1437 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:03<00:00,  9.30it/s, loss=0.195]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 3.98 s | Train Loss: 0.1572 | Val Loss: 0.1276 | Val Accuracy: 0.9595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 19.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA ---\n",
            "Training time: 21.09 seconds\n",
            "Test Loss: 0.1195 | Test Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▁▄▇█</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▄▂▂▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁█</td></tr><tr><td>val_loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>3.98083</td></tr><tr><td>model_type</td><td>LoRA</td></tr><tr><td>parameter_efficiency</td><td>1.65</td></tr><tr><td>test_accuracy</td><td>0.9527</td></tr><tr><td>test_loss</td><td>0.11947</td></tr><tr><td>total_parameters</td><td>44845060</td></tr><tr><td>train_loss</td><td>0.15718</td></tr><tr><td>trainable_parameters</td><td>739586</td></tr><tr><td>training_time</td><td>21.09</td></tr><tr><td>val_accuracy</td><td>0.95946</td></tr><tr><td>val_loss</td><td>0.12757</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LoRA_lr2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/g6815405' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/g6815405</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210613-g6815405/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChemBeRTa With AdaLoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "fgg2HPjJfscE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_adalora_model(model_name):\n",
        "    \"\"\"\n",
        "    Set up a model with AdaLoRA configuration\n",
        "    \"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,  # Binary classification for ClinTox\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "    train_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"train\", max_length=MAX_LENGTH)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    ADA_TOTALSTEP = EPOCHS * len(train_dataloader)\n",
        "\n",
        "    peft_config = AdaLoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        total_step = ADA_TOTALSTEP\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model"
      ],
      "metadata": {
        "id": "xaxGII1HstqT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_tuning(\"AdaLoRA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CMey4JZ1f-s5",
        "outputId": "989d4168-1ab7-4926-d591-78194680184d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210638-wdgb6fsm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/wdgb6fsm' target=\"_blank\">AdaLoRA_Hyperparameter_Sweep</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/wdgb6fsm' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/wdgb6fsm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA with learning_rate=1e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_Hyperparameter_Sweep</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/wdgb6fsm' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/wdgb6fsm</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210638-wdgb6fsm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210640-q8md1ruo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/q8md1ruo' target=\"_blank\">AdaLoRA_lr_1e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/q8md1ruo' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/q8md1ruo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_1e-05 - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_1e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/q8md1ruo' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/q8md1ruo</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210640-q8md1ruo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210641-16nd3652</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/16nd3652' target=\"_blank\">AdaLoRA_lr_1e-05_lr1e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/16nd3652' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/16nd3652</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_1e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.454]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.93 s | Train Loss: 0.5482 | Val Loss: 0.3565 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.396]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.93 s | Train Loss: 0.3393 | Val Loss: 0.2361 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.369]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.92 s | Train Loss: 0.2702 | Val Loss: 0.1987 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.269]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.93 s | Train Loss: 0.2403 | Val Loss: 0.1824 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.26it/s, loss=0.215]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.92 s | Train Loss: 0.2253 | Val Loss: 0.1720 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_1e-05 ---\n",
            "Training time: 31.28 seconds\n",
            "Test Loss: 0.1818 | Test Accuracy: 0.9324\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>█▆▄▆▁</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.91913</td></tr><tr><td>model_type</td><td>AdaLoRA_lr_1e-05</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.93243</td></tr><tr><td>test_loss</td><td>0.1818</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.22534</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.28</td></tr><tr><td>val_accuracy</td><td>0.9527</td></tr><tr><td>val_loss</td><td>0.17198</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_1e-05_lr1e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/16nd3652' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/16nd3652</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210641-16nd3652/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA with learning_rate=2e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210719-f0t833z7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/f0t833z7' target=\"_blank\">AdaLoRA_lr_2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/f0t833z7' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/f0t833z7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_2e-05 - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/f0t833z7' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/f0t833z7</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210719-f0t833z7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210719-rzali9no</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/rzali9no' target=\"_blank\">AdaLoRA_lr_2e-05_lr2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/rzali9no' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/rzali9no</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_2e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.32it/s, loss=0.254]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.86 s | Train Loss: 0.4113 | Val Loss: 0.2201 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.29it/s, loss=0.25]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.89 s | Train Loss: 0.2527 | Val Loss: 0.1799 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.26it/s, loss=0.0989]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.91 s | Train Loss: 0.2225 | Val Loss: 0.1664 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.175]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.93 s | Train Loss: 0.1976 | Val Loss: 0.1569 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.27it/s, loss=0.306]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.91 s | Train Loss: 0.1853 | Val Loss: 0.1492 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_2e-05 ---\n",
            "Training time: 31.16 seconds\n",
            "Test Loss: 0.1439 | Test Accuracy: 0.9459\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▄▆█▅</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.90522</td></tr><tr><td>model_type</td><td>AdaLoRA_lr_2e-05</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.94595</td></tr><tr><td>test_loss</td><td>0.14386</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.18532</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.16</td></tr><tr><td>val_accuracy</td><td>0.9527</td></tr><tr><td>val_loss</td><td>0.14919</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_2e-05_lr2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/rzali9no' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/rzali9no</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210719-rzali9no/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA with learning_rate=3e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210756-abjf5l5b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/abjf5l5b' target=\"_blank\">AdaLoRA_lr_3e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/abjf5l5b' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/abjf5l5b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_3e-05 - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_3e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/abjf5l5b' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/abjf5l5b</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210756-abjf5l5b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210757-4u5e0vge</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/4u5e0vge' target=\"_blank\">AdaLoRA_lr_3e-05_lr3e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/4u5e0vge' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/4u5e0vge</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_3e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.26it/s, loss=0.182]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.91 s | Train Loss: 0.3818 | Val Loss: 0.1929 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.217]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.93 s | Train Loss: 0.2297 | Val Loss: 0.1687 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.284]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.93 s | Train Loss: 0.1976 | Val Loss: 0.1554 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.267]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.92 s | Train Loss: 0.1758 | Val Loss: 0.1454 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.276]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.93 s | Train Loss: 0.1632 | Val Loss: 0.1379 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_3e-05 ---\n",
            "Training time: 31.28 seconds\n",
            "Test Loss: 0.1256 | Test Accuracy: 0.9459\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▆█▄▇</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.9289</td></tr><tr><td>model_type</td><td>AdaLoRA_lr_3e-05</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.94595</td></tr><tr><td>test_loss</td><td>0.12557</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.16315</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.28</td></tr><tr><td>val_accuracy</td><td>0.9527</td></tr><tr><td>val_loss</td><td>0.13793</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_3e-05_lr3e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/4u5e0vge' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/4u5e0vge</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210757-4u5e0vge/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA with learning_rate=5e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210835-yneznr3t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/yneznr3t' target=\"_blank\">AdaLoRA_lr_5e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/yneznr3t' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/yneznr3t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_5e-05 - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_5e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/yneznr3t' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/yneznr3t</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210835-yneznr3t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210836-5xjmjzzd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/5xjmjzzd' target=\"_blank\">AdaLoRA_lr_5e-05_lr5e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/5xjmjzzd' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/5xjmjzzd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_5e-05 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.28it/s, loss=0.0596]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.90 s | Train Loss: 0.3224 | Val Loss: 0.1688 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.26it/s, loss=0.173]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.91 s | Train Loss: 0.1955 | Val Loss: 0.1484 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.27it/s, loss=0.116]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.90 s | Train Loss: 0.1624 | Val Loss: 0.1367 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.14]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.93 s | Train Loss: 0.1437 | Val Loss: 0.1279 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.258]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.93 s | Train Loss: 0.1307 | Val Loss: 0.1205 | Val Accuracy: 0.9595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_5e-05 ---\n",
            "Training time: 31.22 seconds\n",
            "Test Loss: 0.1070 | Test Accuracy: 0.9459\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▄▂██</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁█</td></tr><tr><td>val_loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.93041</td></tr><tr><td>model_type</td><td>AdaLoRA_lr_5e-05</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.94595</td></tr><tr><td>test_loss</td><td>0.10697</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.13065</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.22</td></tr><tr><td>val_accuracy</td><td>0.95946</td></tr><tr><td>val_loss</td><td>0.12054</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_5e-05_lr5e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/5xjmjzzd' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/5xjmjzzd</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210836-5xjmjzzd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA with learning_rate=0.0001 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210913-aicbkymy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/aicbkymy' target=\"_blank\">AdaLoRA_lr_0.0001</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/aicbkymy' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/aicbkymy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_0.0001 - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_0.0001</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/aicbkymy' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/aicbkymy</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210913-aicbkymy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210914-5u8h45bg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/5u8h45bg' target=\"_blank\">AdaLoRA_lr_0.0001_lr0.0001</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/5u8h45bg' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/5u8h45bg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_0.0001 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.27it/s, loss=0.426]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.91 s | Train Loss: 0.2548 | Val Loss: 0.1478 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.0596]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.93 s | Train Loss: 0.1558 | Val Loss: 0.1298 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.0502]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.92 s | Train Loss: 0.1305 | Val Loss: 0.1172 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.149]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.93 s | Train Loss: 0.1155 | Val Loss: 0.1043 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.23it/s, loss=0.0337]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.94 s | Train Loss: 0.0970 | Val Loss: 0.0941 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_0.0001 ---\n",
            "Training time: 31.28 seconds\n",
            "Test Loss: 0.0832 | Test Accuracy: 0.9595\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▅▅▇█</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▄▂▂▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁███</td></tr><tr><td>val_loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.9399</td></tr><tr><td>model_type</td><td>AdaLoRA_lr_0.0001</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.95946</td></tr><tr><td>test_loss</td><td>0.08322</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.09696</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.28</td></tr><tr><td>val_accuracy</td><td>0.96622</td></tr><tr><td>val_loss</td><td>0.09406</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_0.0001_lr0.0001</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/5u8h45bg' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/5u8h45bg</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210914-5u8h45bg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA with learning_rate=0.0003 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210951-gn5bobjr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/gn5bobjr' target=\"_blank\">AdaLoRA_lr_0.0003</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/gn5bobjr' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/gn5bobjr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_0.0003 - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_0.0003</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/gn5bobjr' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/gn5bobjr</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210951-gn5bobjr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_210952-fripp6iw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/fripp6iw' target=\"_blank\">AdaLoRA_lr_0.0003_lr0.0003</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/fripp6iw' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/fripp6iw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_0.0003 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.28it/s, loss=0.141]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.90 s | Train Loss: 0.2132 | Val Loss: 0.1242 | Val Accuracy: 0.9595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.26it/s, loss=0.0983]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.91 s | Train Loss: 0.1220 | Val Loss: 0.1129 | Val Accuracy: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.111]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.92 s | Train Loss: 0.0970 | Val Loss: 0.0778 | Val Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.0748]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.93 s | Train Loss: 0.0629 | Val Loss: 0.0793 | Val Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.25it/s, loss=0.00538]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.93 s | Train Loss: 0.0560 | Val Loss: 0.0789 | Val Accuracy: 0.9730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA_lr_0.0003 ---\n",
            "Training time: 31.24 seconds\n",
            "Test Loss: 0.0597 | Test Accuracy: 0.9797\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▄▆█▇</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▃██▆</td></tr><tr><td>val_loss</td><td>█▆▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.92741</td></tr><tr><td>model_type</td><td>AdaLoRA_lr_0.0003</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.97973</td></tr><tr><td>test_loss</td><td>0.05974</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.05596</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.24</td></tr><tr><td>val_accuracy</td><td>0.97297</td></tr><tr><td>val_loss</td><td>0.07885</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr_0.0003_lr0.0003</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/fripp6iw' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/fripp6iw</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_210952-fripp6iw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_211028-6lu41hd8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/6lu41hd8' target=\"_blank\">AdaLoRA_Hyperparameter_GraphTable</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/6lu41hd8' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/6lu41hd8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_Hyperparameter_GraphTable</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/6lu41hd8' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/6lu41hd8</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_211028-6lu41hd8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   learning_rate  test_accuracy  training_time\n",
              "0        0.00001       0.932432          31.28\n",
              "1        0.00002       0.945946          31.16\n",
              "2        0.00003       0.945946          31.28\n",
              "3        0.00005       0.945946          31.22\n",
              "4        0.00010       0.959459          31.28\n",
              "5        0.00030       0.979730          31.24"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f86e7fd-ec09-4f12-8e98-11646aa4c372\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>training_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.932432</td>\n",
              "      <td>31.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00002</td>\n",
              "      <td>0.945946</td>\n",
              "      <td>31.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00003</td>\n",
              "      <td>0.945946</td>\n",
              "      <td>31.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00005</td>\n",
              "      <td>0.945946</td>\n",
              "      <td>31.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.959459</td>\n",
              "      <td>31.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00030</td>\n",
              "      <td>0.979730</td>\n",
              "      <td>31.24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f86e7fd-ec09-4f12-8e98-11646aa4c372')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f86e7fd-ec09-4f12-8e98-11646aa4c372 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f86e7fd-ec09-4f12-8e98-11646aa4c372');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-76a55f4c-b880-479a-881d-aca4a9b43f1f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76a55f4c-b880-479a-881d-aca4a9b43f1f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-76a55f4c-b880-479a-881d-aca4a9b43f1f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"hyperparameter_tuning(\\\"AdaLoRA\\\")\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00011004544515789829,\n        \"min\": 1e-05,\n        \"max\": 0.0003,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1e-05,\n          2e-05,\n          0.0003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016225597886781858,\n        \"min\": 0.9324324324324325,\n        \"max\": 0.9797297297297297,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9459459459459459,\n          0.9797297297297297,\n          0.9324324324324325\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"training_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.048027769744874944,\n        \"min\": 31.16,\n        \"max\": 31.28,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          31.16,\n          31.24,\n          31.28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model_adalora = setup_adalora_model(MODEL_NAME)\n",
        "\n",
        "adalora_metrics = train_and_profile(model_adalora, tokenizer, \"AdaLoRA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Szf3L-BDXmqT",
        "outputId": "26e0e664-6902-4588-97be-9fe4409ae761"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA - Trainable Parameters ---\n",
            "Trainable: 1606335 | All: 45730290 | % Trained: 3.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_211032-gf0yx2ln</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/gf0yx2ln' target=\"_blank\">AdaLoRA_lr2e-05</a></strong> to <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/gf0yx2ln' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/gf0yx2ln</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 37/37 [00:05<00:00,  6.27it/s, loss=0.308]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Time: 5.90 s | Train Loss: 0.4922 | Val Loss: 0.2395 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 37/37 [00:05<00:00,  6.26it/s, loss=0.148]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 14.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Training Time: 5.92 s | Train Loss: 0.2626 | Val Loss: 0.1835 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.139]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Training Time: 5.93 s | Train Loss: 0.2281 | Val Loss: 0.1679 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 37/37 [00:05<00:00,  6.24it/s, loss=0.18]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Training Time: 5.93 s | Train Loss: 0.2046 | Val Loss: 0.1585 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 37/37 [00:05<00:00,  6.23it/s, loss=0.202]\n",
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Training Time: 5.94 s | Train Loss: 0.1893 | Val Loss: 0.1510 | Val Accuracy: 0.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AdaLoRA ---\n",
            "Training time: 31.29 seconds\n",
            "Test Loss: 0.1446 | Test Accuracy: 0.9459\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▄▇▆█</td></tr><tr><td>parameter_efficiency</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>total_parameters</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>trainable_parameters</td><td>▁▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>5.94015</td></tr><tr><td>model_type</td><td>AdaLoRA</td></tr><tr><td>parameter_efficiency</td><td>3.51</td></tr><tr><td>test_accuracy</td><td>0.94595</td></tr><tr><td>test_loss</td><td>0.14459</td></tr><tr><td>total_parameters</td><td>45730290</td></tr><tr><td>train_loss</td><td>0.1893</td></tr><tr><td>trainable_parameters</td><td>1606335</td></tr><tr><td>training_time</td><td>31.29</td></tr><tr><td>val_accuracy</td><td>0.9527</td></tr><tr><td>val_loss</td><td>0.151</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">AdaLoRA_lr2e-05</strong> at: <a href='https://wandb.ai/hpml-proj-deepchem/testing/runs/gf0yx2ln' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing/runs/gf0yx2ln</a><br> View project at: <a href='https://wandb.ai/hpml-proj-deepchem/testing' target=\"_blank\">https://wandb.ai/hpml-proj-deepchem/testing</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_211032-gf0yx2ln/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(baseline_metrics, lora_metrics, adalora_metrics):\n",
        "    \"\"\"\n",
        "    Log a bar chart comparing training time, test accuracy, and parameter count for all models.\n",
        "    \"\"\"\n",
        "    wandb.init(project=\"testing\", entity=\"hpml-proj-deepchem\", name=\"Model_Comparison\")\n",
        "    models = [\"Baseline\", \"LoRA\", \"AdaLoRA\"]\n",
        "    test_accuracies = [baseline_metrics[\"test_accuracy\"], lora_metrics[\"test_accuracy\"], adalora_metrics[\"test_accuracy\"]]\n",
        "    training_times = [baseline_metrics[\"training_time\"], lora_metrics[\"training_time\"], adalora_metrics[\"training_time\"]]\n",
        "    param_counts = [baseline_metrics[\"trainable_parameters\"], lora_metrics[\"trainable_parameters\"], adalora_metrics[\"trainable_parameters\"]]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    axs[0].bar(models, test_accuracies, color=[\"blue\", \"green\", \"red\"])\n",
        "    axs[0].set_title(\"Test Accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[1].bar(models, training_times, color=[\"blue\", \"green\", \"red\"])\n",
        "    axs[1].set_title(\"Training Time (s)\")\n",
        "    axs[1].set_ylabel(\"Seconds\")\n",
        "    axs[2].bar(models, param_counts, color=[\"blue\", \"green\", \"red\"])\n",
        "    axs[2].set_title(\"Trainable Parameters\")\n",
        "    axs[2].set_ylabel(\"Count\")\n",
        "    axs[2].set_yscale(\"log\")\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"model_comparison\": wandb.Image(fig)})\n",
        "    plt.close(fig)\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "id": "MuN6Du-cykRj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McmEu5cQuHR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}