{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m7ANaq05Ehz_"
      },
      "outputs": [],
      "source": [
        "!pip install deepchem transformers peft onnxruntime onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import deepchem as dc\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rdkit import Chem\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ],
      "metadata": {
        "id": "c-cd2htkEoYk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths and parameters\n",
        "MODEL_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# LoRA Configuration\n",
        "LORA_R = 8  # Rank of LoRA\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1"
      ],
      "metadata": {
        "id": "D6okUAyZErW7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClinToxDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, split='train', max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "\n",
        "        # Load ClinTox dataset from DeepChem\n",
        "        tasks, datasets, transformers = dc.molnet.load_clintox()\n",
        "        train_dataset, valid_dataset, test_dataset = datasets\n",
        "\n",
        "        # Convert to SMILES and labels\n",
        "        self.smiles_train, self.labels_train = self.remove_invalid_smiles(train_dataset.ids, train_dataset.y)\n",
        "        self.smiles_valid, self.labels_valid = self.remove_invalid_smiles(valid_dataset.ids, valid_dataset.y)\n",
        "        self.smiles_test, self.labels_test = self.remove_invalid_smiles(test_dataset.ids, test_dataset.y)\n",
        "\n",
        "        # Set active split\n",
        "        if split == 'train':\n",
        "            self.smiles = self.smiles_train\n",
        "            self.labels = self.labels_train\n",
        "        elif split == 'valid':\n",
        "            self.smiles = self.smiles_valid\n",
        "            self.labels = self.labels_valid\n",
        "        elif split == 'test':\n",
        "            self.smiles = self.smiles_test\n",
        "            self.labels = self.labels_test\n",
        "        else:\n",
        "            raise ValueError(\"Invalid split. Use 'train', 'valid', or 'test'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "    def remove_invalid_smiles(self, smiles, labels):\n",
        "        valid_indices = []\n",
        "        for i, smile in enumerate(smiles):\n",
        "            try:\n",
        "                mol = Chem.MolFromSmiles(smile)\n",
        "                if mol is not None:\n",
        "                    valid_indices.append(i)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return smiles[valid_indices], labels[valid_indices]"
      ],
      "metadata": {
        "id": "bsRwSyDeEuzD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_lora_model(model_name):\n",
        "    \"\"\"\n",
        "    Set up a model with LoRA configuration\n",
        "    \"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,  # Binary classification for ClinTox\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        target_modules=[\"query\", \"value\"]  # Target attention modules\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model"
      ],
      "metadata": {
        "id": "UjwQY4ggE9sX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer):\n",
        "    # Create dataset for train split\n",
        "    train_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"train\", max_length=MAX_LENGTH)\n",
        "\n",
        "    # Create dataloader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EdeEQJwJFD9J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export(model, tokenizer, save_path, quantize=False):\n",
        "    \"\"\"\n",
        "    Export a HuggingFace model to ONNX format. Optionally apply dynamic quantization.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model to export.\n",
        "        tokenizer: Associated tokenizer.\n",
        "        save_path: Path to save the ONNX (or quantized) model.\n",
        "        quantize: Whether to apply dynamic quantization (QInt8).\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Example input for tracing\n",
        "    encoded = tokenizer(\n",
        "        \"CC(=O)Oc1ccccc1C(=O)O\",  # SMILES example\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"].to(device)\n",
        "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Use temp path if quantizing\n",
        "    export_path = \"temp_model.onnx\" if quantize else save_path\n",
        "\n",
        "    # Export to ONNX\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        (input_ids, attention_mask),\n",
        "        export_path,\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\"input_ids\": {0: \"batch\"}, \"attention_mask\": {0: \"batch\"}},\n",
        "        opset_version=14\n",
        "    )\n",
        "\n",
        "    # Quantize if requested\n",
        "    if quantize:\n",
        "        quantize_dynamic(export_path, save_path, weight_type=QuantType.QInt8)\n",
        "        os.remove(export_path)\n",
        "        print(f\"Quantized model saved to: {save_path}\")\n",
        "    else:\n",
        "        print(f\"Baseline ONNX model saved to: {save_path}\")\n",
        "\n",
        "    # Report model size\n",
        "    size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
        "    print(f\"Model size: {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "wfWsS0dlIKLC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_onnx_model(onnx_path):\n",
        "    session = ort.InferenceSession(onnx_path)\n",
        "\n",
        "    test_dataset = ClinToxDataset(\"clintox\", tokenizer, split=\"test\", max_length=MAX_LENGTH)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids'].numpy()\n",
        "        attention_mask = batch['attention_mask'].numpy()\n",
        "        labels = batch['labels'].numpy()\n",
        "\n",
        "        # Run inference with ONNX\n",
        "        ort_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }\n",
        "        ort_outs = session.run(None, ort_inputs)  # Returns a list; usually one element: logits\n",
        "\n",
        "        logits = ort_outs[0]\n",
        "        probs = 1 / (1 + np.exp(-logits))  # Sigmoid manually\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Total Inference Time: {total_time:.2f} s\")"
      ],
      "metadata": {
        "id": "_ffxrKBCJ6--"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = setup_lora_model(MODEL_NAME)\n",
        "\n",
        "trained_model = train(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuYQ7hFJQCRf",
        "outputId": "e15177f0-7add-4abd-f5cc-cdcb9e4a0947"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/5: 100%|██████████| 37/37 [00:02<00:00, 15.51it/s, loss=0.198]\n",
            "Epoch 2/5: 100%|██████████| 37/37 [00:02<00:00, 15.88it/s, loss=0.228]\n",
            "Epoch 3/5: 100%|██████████| 37/37 [00:02<00:00, 15.85it/s, loss=0.269]\n",
            "Epoch 4/5: 100%|██████████| 37/37 [00:02<00:00, 15.90it/s, loss=0.125]\n",
            "Epoch 5/5: 100%|██████████| 37/37 [00:02<00:00, 15.92it/s, loss=0.154]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export(trained_model, tokenizer, save_path=\"model-baseline.onnx\", quantize=False)\n",
        "\n",
        "evaluate_onnx_model(\"model-baseline.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvSXUb9CR8nx",
        "outputId": "9e61e9f7-1591-431b-f203-f25a47ba2375"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline ONNX model saved to: model-baseline.onnx\n",
            "Model size: 169.02 MB\n",
            "Test Accuracy: 0.9459\n",
            "Total Inference Time: 5.82 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export(trained_model, tokenizer, save_path=\"model-quant.onnx\", quantize=True)\n",
        "\n",
        "evaluate_onnx_model(\"model-quant.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf3R0NNDKi_L",
        "outputId": "369673ac-fa65-4e8d-aeed-5626cd6b2570"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model saved to: model-quant.onnx\n",
            "Model size: 42.75 MB\n",
            "Test Accuracy: 0.9459\n",
            "Total Inference Time: 2.03 s\n"
          ]
        }
      ]
    }
  ]
}